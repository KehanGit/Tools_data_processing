{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### The notebook is used to download planetscope images with custom requiremnets (Kehan Yang, kyang33@uw.edu)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load packages and set up directories\n",
    "The script is built upon package planet Version 1 and has not been adapted to the recently launched planet Version 2."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import glob\n",
    "from get_planet import *\n",
    "from os.path import exists"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Setup OK: API key valid\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authorize Planet account.\n",
    "You can copy and paste your planet API from your Planet Account setting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# If you're following along with this notebook, you can enter your API Key on the following line, and uncomment it:\n",
    "# os.environ['PLANET_API_KEY']='XX'\n",
    "# Setup the API Key from the `PL_API_KEY` environment variable\n",
    "PLANET_API_KEY = os.getenv('PLANET_API_KEY')\n",
    "\n",
    "#### Get your API Key and run validity check\n",
    "# This gets your API key and prompts you incase your API key is missing or if there are authentication issues\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set up directories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# directory for the geomtry, the format has to be geojson\n",
    "dir_geom = \"../../../research/pc2_meadows/data/Sierra_meadows/geojson_test/\"\n",
    "\n",
    "dir_root = './'\n",
    "# directory where the images will be downloaded. \n",
    "dir_download = dir_root + 'temp/download/'\n",
    "\n",
    "# directory for the download links and image ids \n",
    "dir_order_url =  dir_root + 'temp/links/'\n",
    "\n",
    "# change the flag if search and/or download data are required.\n",
    "flag_search = True\n",
    "flag_order = True\n",
    "flag_download = False\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: dir_download: File exists\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start to search and/or download data\n",
    "If flag_download is set to False, the order will not be placed, and your quote will not be consumed. The total areas will be saved in a CSV file, allowing you to estimate the total size of the areas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# define the searching period\n",
    "ID_period = [str(i) for i in range(2018,2024)]\n",
    "\n",
    "if flag_search:\n",
    "    for yr in ID_period:\n",
    "        df_search = pd.DataFrame() # save all image ids\n",
    "        \n",
    "        #file to store url -- planet data download links\n",
    "        file_orders = dir_order_url + yr + '.txt'\n",
    "        start_time = yr + '-01-01T00:00:00'\n",
    "        end_time = yr + '-12-02T12:00:00'\n",
    "        \n",
    "        overlap = 99 # at least with 99% overlap \n",
    "        cloud_pct = 0.05 # no more than 5% cloud cover\n",
    "\n",
    "\n",
    "        #search for geojson file\n",
    "        fn = glob.glob(dir_geom + \"*json\")\n",
    "        ID_shp = [id.split(\"/\")[-1] for id in fn]\n",
    "        df = pd.DataFrame(data = {\n",
    "            \"file\": fn, \n",
    "            \"index\":  [i.split(\"/\")[-1] for i in fn],\n",
    "            \"ID\": [id.split(\"/\")[-1].split('.')[0] for id in fn]\n",
    "            })\n",
    "        df = df.sort_values(\"index\", ascending = True)\n",
    "\n",
    "        print(df.head())\n",
    "\n",
    "    # check whether the order url txt file is exist. if exist, read data; otherwise, creat file.\n",
    "        idx = 0 \n",
    "        if exists(file_orders):\n",
    "            order_urls = pd.read_csv(file_orders)\n",
    "        else:\n",
    "            order_urls = pd.DataFrame(columns = [\"index\",\"ID_geom\", \"order_url\"])\n",
    "\n",
    "\n",
    "        for irow in df.itertuples():\n",
    "\n",
    "        # Search id \n",
    "            print(irow)\n",
    "            ID_geom = irow.ID.split(\".\")[0]+ '_' + yr\n",
    "            print(ID_geom)\n",
    "\n",
    "            if ID_geom not in order_urls.ID_geom.to_list():\n",
    "\n",
    "                print('Searching available images ------- ')\n",
    "                idlist = ft_iterate(item_type='PSScene', # planet has changed the product item type from 'PSScene4Band' with PSScene\n",
    "                        asset_type= 'ortho_analytic_4b',\n",
    "                        geom = read_geom(irow.file),#\".json\"),\n",
    "                        start = start_time,\n",
    "                        end = end_time,\n",
    "                        cloud_cover = cloud_pct, #cloud cover range 0-1 represting 0-100% so 0.5 means max allowed 50% cloud cover\n",
    "                        ovp = overlap) #% minimum % overlap 0-100\n",
    "\n",
    "                idlist['ID_geom'] = ID_geom\n",
    "                print(idlist.shape)\n",
    "                idlist.sort_values(\"date\")\n",
    "                df_search = pd.concat([df_search, idlist])\n",
    "\n",
    "\n",
    "                # print(irow.file)\n",
    "                if(flag_order):\n",
    "                    payload_info = order_payload(Name_download = ID_geom, ID_imgs = idlist.id.values.tolist(), File_geom = irow.file)\n",
    "                    # print(payload_info)\n",
    "                    print(\"Pay order:\".format(),ID_geom)\n",
    "\n",
    "\n",
    "                    order_url = order_now(payload_info) # error response 400  \n",
    "\n",
    "                    order_urls.loc[idx, \"index\"] = idx        \n",
    "                    order_urls.loc[idx, \"ID_geom\"] = ID_geom\n",
    "                    order_urls.loc[idx, \"order_url\"] = order_url\n",
    "                    order_urls.loc[idx, \"NUM\"] = idlist.shape[0]  \n",
    "                    order_urls.loc[idx, \"Total area\"] = sum(idlist['estimated area'])  \n",
    "\n",
    "\n",
    "                    # order_urls.append(order_url)  # save all URLs\n",
    "                    order_urls.to_csv(file_orders, index = None)# save all URLs\n",
    "\n",
    "\n",
    "            idx = idx + 1\n",
    "        df_search.to_csv(dir_order_url+'idlist.csv')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start to download data\n",
    "After receiving email notifications, you can use the following code to download the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# read order URL from file_orders\n",
    "flag_download = True\n",
    "dir_download = '/Volumes/My Book/data/Farmponds/GIS/Planet_Easternshore/'\n",
    "fn = glob.glob(dir_order_url + '*.txt')\n",
    "fn.sort()\n",
    "\n",
    "if flag_download:\n",
    "    for file_orders in fn:\n",
    "        order_urls_read = pd.read_csv(file_orders)\n",
    "\n",
    "        for url in order_urls_read.itertuples():\n",
    "            print(url.order_url)\n",
    "            # if poll_for_success(url.order_url):\n",
    "            if os.path.exists(dir_download + url.ID_geom):\n",
    "                print(\"Data have been downloaded\".format(), dir_download + url.ID_geom)\n",
    "            else:\n",
    "                print(\"start downloading data to\".format(), dir_download + url.ID_geom)\n",
    "                download_results(url.order_url,folder = dir_download + url.ID_geom)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check downloaded data\n",
    "Check the data to determine if it has been downloaded completely. If not, download the missing data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#check whether all data have been downloaded \n",
    "# read search csv \n",
    "dir_search = '/Users/kehanyang/Documents/resarch/pc2_meadows/data/planet/orders/Meadows/'\n",
    "fn = glob.glob(dir_search + '*.csv')\n",
    "id_miss = []\n",
    "for i in range(0, len(fn)-1):\n",
    "    data = pd.read_csv(fn[i])\n",
    "    id = os.path.basename(fn[i]).split('.csv')[0]\n",
    "    # print(id)\n",
    "    # print(data[[\"id\",'date','instrument']])\n",
    "    data['id_three'] = [(i.split(\"_\")[0] + '_' +  i.split(\"_\")[1] + '_' + i.split(\"_\")[2]) for i in data['id']]\n",
    "\n",
    "    dir_image = dir_search + id\n",
    "    # print(dir_image)\n",
    "    fn_img = glob.glob(dir_image + '/**/**/*.tif', recursive = True)\n",
    "    fn_img_names = [os.path.basename(f) for f in fn_img]\n",
    "    id_downloaded = [(i.split(\"_\")[0] + '_' +  i.split(\"_\")[1] + '_' + i.split(\"_\")[2]) for i in fn_img_names]\n",
    "\n",
    "    not_downloaded = data[~data['id_three'].isin(id_downloaded)]\n",
    "\n",
    "    if len(not_downloaded) > 0:\n",
    "        print(id)\n",
    "        print(not_downloaded)\n",
    "        id_miss.append(id)\n",
    "\n",
    "id_downloaded\n",
    "not_downloaded, id_downloaded\n",
    "print(len(id_miss))\n",
    "id_miss"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('planet_p36': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "interpreter": {
   "hash": "7d941b521942abff02888ea7873cca51c2aac5fb2f3b440dbf15a61d263ddb0d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}