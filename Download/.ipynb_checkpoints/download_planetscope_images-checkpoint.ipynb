{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280a1092-23f1-41f8-b632-442c039d3c2a",
   "metadata": {},
   "source": [
    "### The notebook is used to download planetscope images with custom requiremnets (Kehan Yang, kyang33@uw.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc708a4-c6d0-4f3c-ab46-0a36ce671419",
   "metadata": {},
   "source": [
    "#### Load packages and set up directories\n",
    "Planet has updated its API in March 2023, so some of the functions may not be transferable. This script is based on Planet 1.5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b314445e-3631-4ae8-a117-c31c55fbf8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from get_planet import *\n",
    "from os.path import exists\n",
    "from planet.api.auth import find_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d65faa-e373-4da1-b221-541a4813102a",
   "metadata": {},
   "source": [
    "### Authorize Planet account.\n",
    "You can copy and paste your planet API from your Planet Account setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27767b2c-5e26-49aa-bf17-f1756c65bc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup OK: API key valid\n"
     ]
    }
   ],
   "source": [
    "# If you're following along with this notebook, you can enter your API Key on the following line, and uncomment it:\n",
    "os.environ['PL_API_KEY']='XX'\n",
    "\n",
    "# Setup the API Key from the `PL_API_KEY` environment variable\n",
    "PLANET_API_KEY = os.getenv('PL_API_KEY')\n",
    "\n",
    "#### Get your API Key and run validity check\n",
    "# This gets your API key and prompts you incase your API key is missing or if there are authentication issues\n",
    "\n",
    "## Get your API Key\n",
    "try:\n",
    "    PLANET_API_KEY = find_api_key() #remove find_api_key and place your api key like 'api-key'\n",
    "except Exception as e:\n",
    "    print(\"Failed to get Planet Key: Try planet init or install Planet Command line tool\")\n",
    "    sys.exit()\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "# check if API key is valid \n",
    "response = requests.get('https://api.planet.com/compute/ops/orders/v2',auth=(PLANET_API_KEY, \"\"))\n",
    "if response.status_code==200:\n",
    "    print('Setup OK: API key valid')\n",
    "else:\n",
    "    print(f'Failed with response code {response.status_code}: reinitialize using planet init')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4923eba-4b65-4793-928a-6345be739b11",
   "metadata": {},
   "source": [
    "### Set up directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99647d48-8fa1-4435-b24e-a6d62b318c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory for the geomtry, the format has to be geojson\n",
    "dir_geom = \"/Users/kehanyang/Documents/resarch/pc2_meadows/data/snotel/meadow_selected_extent_geojson_download2/\"\n",
    "# dir_geom = '../../Alaska_project/data/GIS/'\n",
    "dir_geom = '../../FFLake/Farmponds/data/GIS/single/'\n",
    "\n",
    "# directory where the images will be stored. \n",
    "dir_download = \"/Users/kehanyang/Documents/resarch/pc2_meadows/data/planet/orders/Meadows/\"\n",
    "# dir_meadow_images = \"/Users/kehanyang/Documents/resarch/pc2_meadows/data/planet/orders/Meadows/\"\n",
    "\n",
    "# directory for the download links and image ids \n",
    "dir_order_url =  '../../Alaska_project/data/planet/'\n",
    "dir_order_url =  '../../FFLake/Farmponds/data/GIS/'\n",
    "\n",
    "# change the flag if search and/or download data are required.\n",
    "flag_search = False\n",
    "flag_order = False\n",
    "flag_download = False\n",
    "\n",
    "\n",
    "# \n",
    "ID_period = '2016'\n",
    "#file to store url -- planet data download links\n",
    "file_orders = dir_order_url+ID_period+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f95b9e9-fed8-4320-8b08-4d569a81a83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>instrument</th>\n",
       "      <th>estimated area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200223_154148_0f52</td>\n",
       "      <td>20200223</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200223_160139_60_1059</td>\n",
       "      <td>20200223</td>\n",
       "      <td>PS2.SD</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200222_154222_1011</td>\n",
       "      <td>20200222</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200217_153931_103e</td>\n",
       "      <td>20200217</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200215_154434_0f34</td>\n",
       "      <td>20200215</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20200208_135815_1_0f33</td>\n",
       "      <td>20200208</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20200209_140234_0f2e</td>\n",
       "      <td>20200209</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20200126_154426_0f34</td>\n",
       "      <td>20200126</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20200121_155403_41_106a</td>\n",
       "      <td>20200121</td>\n",
       "      <td>PS2.SD</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20200121_154236_1027</td>\n",
       "      <td>20200121</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20200120_153917_1003</td>\n",
       "      <td>20200120</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20200120_153913_103e</td>\n",
       "      <td>20200120</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20200119_154203_1032</td>\n",
       "      <td>20200119</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20200119_140135_104e</td>\n",
       "      <td>20200119</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20200116_154901_1105</td>\n",
       "      <td>20200116</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20200116_153902_1003</td>\n",
       "      <td>20200116</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20200112_140450_1050</td>\n",
       "      <td>20200112</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20200112_140449_1050</td>\n",
       "      <td>20200112</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20200112_153846_103e</td>\n",
       "      <td>20200112</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20200109_140606_1048</td>\n",
       "      <td>20200109</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20200109_154122_1011</td>\n",
       "      <td>20200109</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20200106_140601_0f24</td>\n",
       "      <td>20200106</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20200101_140415_0f4d</td>\n",
       "      <td>20200101</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20200101_154035_1011</td>\n",
       "      <td>20200101</td>\n",
       "      <td>PS2</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id      date instrument  estimated area\n",
       "0      20200223_154148_0f52  20200223        PS2           0.117\n",
       "1   20200223_160139_60_1059  20200223     PS2.SD           0.117\n",
       "2      20200222_154222_1011  20200222        PS2           0.117\n",
       "3      20200217_153931_103e  20200217        PS2           0.117\n",
       "4      20200215_154434_0f34  20200215        PS2           0.117\n",
       "5    20200208_135815_1_0f33  20200208        PS2           0.117\n",
       "6      20200209_140234_0f2e  20200209        PS2           0.117\n",
       "7      20200126_154426_0f34  20200126        PS2           0.117\n",
       "8   20200121_155403_41_106a  20200121     PS2.SD           0.117\n",
       "9      20200121_154236_1027  20200121        PS2           0.117\n",
       "10     20200120_153917_1003  20200120        PS2           0.117\n",
       "11     20200120_153913_103e  20200120        PS2           0.117\n",
       "12     20200119_154203_1032  20200119        PS2           0.117\n",
       "13     20200119_140135_104e  20200119        PS2           0.117\n",
       "14     20200116_154901_1105  20200116        PS2           0.117\n",
       "15     20200116_153902_1003  20200116        PS2           0.117\n",
       "16     20200112_140450_1050  20200112        PS2           0.117\n",
       "17     20200112_140449_1050  20200112        PS2           0.117\n",
       "18     20200112_153846_103e  20200112        PS2           0.117\n",
       "19     20200109_140606_1048  20200109        PS2           0.117\n",
       "20     20200109_154122_1011  20200109        PS2           0.117\n",
       "21     20200106_140601_0f24  20200106        PS2           0.117\n",
       "22     20200101_140415_0f4d  20200101        PS2           0.117\n",
       "23     20200101_154035_1011  20200101        PS2           0.117"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc75a64-c5f9-4781-a732-95ad53e2e46f",
   "metadata": {},
   "source": [
    "### Start to search and/or download data\n",
    "If flag_download is set to False, the order will not be placed, and your quote will not be consumed. The total areas will be saved in a CSV file, allowing you to estimate the total size of the areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e371d08-7778-41ec-8322-a458a942ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_search:\n",
    "    df_search = pd.DataFrame() # save all image ids\n",
    "    # start_time = ID_period + '-01-01T00:00:00'\n",
    "    start_time = '2016-01-01T00:00:00'\n",
    "    # end_time = '2021-12-31T12:00:00'\n",
    "    # end_time = ID_period + '-12-02T12:00:00'\n",
    "    end_time = '2022-12-02T12:00:00'\n",
    "    overlap = 99 # at least with 99% overlap \n",
    "    cloud_pct = 0.05 # no more than 5% cloud cover\n",
    "     \n",
    "    \n",
    "    #search for geojson file\n",
    "    fn = glob.glob(dir_geom + \"*geojson\")\n",
    "    ID_shp = [id.split(\"/\")[-1] for id in fn]\n",
    "    df = pd.DataFrame(data = {\n",
    "        \"file\": fn, \n",
    "        \"index\":  [i.split(\"/\")[-1] for i in fn],\n",
    "        \"ID\": [id.split(\"/\")[-1].split('.')[0] for id in fn]\n",
    "        })\n",
    "    df = df.sort_values(\"index\", ascending = True)\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "# check whether the order url txt file is exist. if exist, read data; otherwise, creat file.\n",
    "    idx = 0 \n",
    "    if exists(file_orders):\n",
    "        order_urls = pd.read_csv(file_orders)\n",
    "    else:\n",
    "        order_urls = pd.DataFrame(columns = {\"index\",\"ID_geom\", \"order_url\"})\n",
    "\n",
    "\n",
    "    for irow in df.itertuples():\n",
    "    \n",
    "    # Search id \n",
    "        print(irow)\n",
    "        ID_geom = irow.ID.split(\".\")[0]+ '_' + ID_period\n",
    "        print(ID_geom)\n",
    "\n",
    "        if ID_geom not in order_urls.ID_geom.to_list():\n",
    "\n",
    "            print('Searching available images ------- ')\n",
    "            idlist = ft_iterate(item_type='PSScene', # planet has changed the product item type from 'PSScene4Band' with PSScene\n",
    "                    asset_type= 'ortho_analytic_4b',\n",
    "                    geom = read_geom(irow.file),#\".json\"),\n",
    "                    start = start_time,\n",
    "                    end = end_time,\n",
    "                    cloud_cover = cloud_pct, #cloud cover range 0-1 represting 0-100% so 0.5 means max allowed 50% cloud cover\n",
    "                    ovp = overlap) #% minimum % overlap 0-100\n",
    "\n",
    "            idlist['ID_geom'] = ID_geom\n",
    "            print(idlist.shape)\n",
    "            idlist.sort_values(\"date\")\n",
    "            df_search = pd.concat([df_search, idlist])\n",
    "            \n",
    "            \n",
    "            # print(irow.file)\n",
    "            if(flag_order):\n",
    "                payload_info = order_payload(Name_download = ID_geom, ID_imgs = idlist.id.values.tolist(), File_geom = irow.file)\n",
    "                # print(payload_info)\n",
    "                print(\"Pay order:\".format(),ID_geom)\n",
    "\n",
    "\n",
    "                order_url = order_now(payload_info) # error response 400  \n",
    "\n",
    "                order_urls.loc[idx, \"index\"] = idx        \n",
    "                order_urls.loc[idx, \"ID_geom\"] = ID_geom\n",
    "                order_urls.loc[idx, \"order_url\"] = order_url\n",
    "\n",
    "\n",
    "                order_urls.append(order_url)  # save all URLs\n",
    "                order_urls.to_csv(file_orders, index = None)# save all URLs\n",
    "\n",
    "            \n",
    "        idx = idx + 1\n",
    "    df_search.to_csv(dir_order_url+'idlist.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7ac2fc2-b69a-4ea3-a38d-478b1bd381fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6887.057"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_search['estimated area'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f1151-a5b8-4527-9b0f-3a3b0c1efbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71852940-a8f6-4118-84c2-99b4b641c416",
   "metadata": {},
   "source": [
    "### Start to download data\n",
    "After receiving email notifications, you can use the following code to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93321992-a966-40f2-9eb9-78de641d5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read order URL from file_orders\n",
    "\n",
    "if flag_download:\n",
    "    order_urls_read = pd.read_csv(file_orders)\n",
    "\n",
    "    for url in order_urls_read.itertuples():\n",
    "        print(url.order_url)\n",
    "        # if poll_for_success(url.order_url):\n",
    "        if os.path.exists(dir_download + url.ID_geom):\n",
    "            print(\"Data have been downloaded\".format(), dir_download + url.ID_geom)\n",
    "        else:\n",
    "            print(\"start downloading data to\".format(), dir_download + url.ID_geom)\n",
    "            download_results(url.order_url,folder = dir_download + url.ID_geom)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b04de5-ccb4-4da3-997c-7b07874c2867",
   "metadata": {},
   "source": [
    "### Check downloaded data\n",
    "Check the data to determine if it has been downloaded completely. If not, download the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba121ba-4533-4d7c-b98e-06217e152d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check whether all data have been downloaded \n",
    "# read search csv \n",
    "dir_search = '/Users/kehanyang/Documents/resarch/pc2_meadows/data/planet/orders/Meadows/'\n",
    "fn = glob.glob(dir_search + '*.csv')\n",
    "id_miss = []\n",
    "for i in range(0, len(fn)-1):\n",
    "    data = pd.read_csv(fn[i])\n",
    "    id = os.path.basename(fn[i]).split('.csv')[0]\n",
    "    # print(id)\n",
    "    # print(data[[\"id\",'date','instrument']])\n",
    "    data['id_three'] = [(i.split(\"_\")[0] + '_' +  i.split(\"_\")[1] + '_' + i.split(\"_\")[2]) for i in data['id']]\n",
    "\n",
    "    dir_image = dir_search + id\n",
    "    # print(dir_image)\n",
    "    fn_img = glob.glob(dir_image + '/**/**/*.tif', recursive = True)\n",
    "    fn_img_names = [os.path.basename(f) for f in fn_img]\n",
    "    id_downloaded = [(i.split(\"_\")[0] + '_' +  i.split(\"_\")[1] + '_' + i.split(\"_\")[2]) for i in fn_img_names]\n",
    "\n",
    "    not_downloaded = data[~data['id_three'].isin(id_downloaded)]\n",
    "\n",
    "    if len(not_downloaded) > 0:\n",
    "        print(id)\n",
    "        print(not_downloaded)\n",
    "        id_miss.append(id)\n",
    "\n",
    "id_downloaded\n",
    "not_downloaded, id_downloaded\n",
    "print(len(id_miss))\n",
    "id_miss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet_project",
   "language": "python",
   "name": "planet_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
